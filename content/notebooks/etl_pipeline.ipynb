{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04ff0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "import time\n",
    "from API_KEY import API_KEY\n",
    "\n",
    "class bronze:\n",
    "\n",
    "    API_KEY = API_KEY\n",
    "    url = \"https://youtube.googleapis.com/youtube/v3/search\"\n",
    "\n",
    "    looger = logging.getLogger(__name__)\n",
    "\n",
    "    def __init__(self, q, regionCode, type, maxresults):\n",
    "        self.API_KEY = bronze.API_KEY\n",
    "        self.url = bronze.url\n",
    "        self.part = \"snippet\"\n",
    "        self.q = q\n",
    "        self.type = type\n",
    "        self.regionCode = regionCode\n",
    "        self.maxresults = maxresults\n",
    "        self.total_results = 0\n",
    "        self.quota_limit = 10000  # Daily quota limit\n",
    "        self.quota_used = 0\n",
    "\n",
    "    def request_data(self, max_channels: int = 1000000) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Function to request data from YouTube API with pagination support\n",
    "        Args:\n",
    "            max_channels (int): Maximum number of channels to retrieve\n",
    "        Returns:\n",
    "            Dict: Combined results from all API requests\n",
    "        \"\"\"\n",
    "        all_items = []\n",
    "        next_page_token = None\n",
    "        params = {\n",
    "            \"part\": self.part,\n",
    "            \"q\": self.q,\n",
    "            \"type\": self.type,\n",
    "            \"regionCode\": self.regionCode,\n",
    "            \"maxResults\": min(50, self.maxresults),  # YouTube API limit is 50 per request\n",
    "            \"key\": self.API_KEY\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            while len(all_items) < max_channels:\n",
    "                if next_page_token:\n",
    "                    params['pageToken'] = next_page_token\n",
    "\n",
    "                self.looger.info(f\"Sending request to YouTube API. Total items so far: {len(all_items)}\")\n",
    "                request = requests.get(self.url, params=params)\n",
    "                \n",
    "                # Check for quota exhaustion (403 status code)\n",
    "                if request.status_code == 403:\n",
    "                    self.looger.warning(\"API quota exhausted. Waiting for quota reset...\")\n",
    "                    time.sleep(3600)  # Wait for 1 hour\n",
    "                    continue\n",
    "\n",
    "                if request.status_code != 200:\n",
    "                    self.looger.error(f\"Request failed with status code {request.status_code}\")\n",
    "                    break\n",
    "\n",
    "                response_data = request.json()\n",
    "                items = response_data.get('items', [])\n",
    "                \n",
    "                if not items:\n",
    "                    self.looger.info(\"No more items found\")\n",
    "                    break\n",
    "\n",
    "                all_items.extend(items)\n",
    "                self.looger.info(f\"Retrieved {len(items)} items. Total: {len(all_items)}\")\n",
    "\n",
    "                next_page_token = response_data.get('nextPageToken')\n",
    "                if not next_page_token:\n",
    "                    self.looger.info(\"No more pages available\")\n",
    "                    break\n",
    "\n",
    "                time.sleep(0.5)\n",
    "\n",
    "                if len(all_items) >= max_channels:\n",
    "                    self.looger.info(f\"Reached maximum number of channels: {max_channels}\")\n",
    "                    break\n",
    "\n",
    "            self.total_results = len(all_items)\n",
    "            self.looger.info(f\"Total channels retrieved: {self.total_results}\")\n",
    "            return {\"items\": all_items[:max_channels]}\n",
    "\n",
    "        except Exception as e:\n",
    "            self.looger.error(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    def save_data(self, data, filename=\"../data/bronze/bronze_data.json\"):\n",
    "        \"\"\"\n",
    "        Function to save data to a JSON file\n",
    "        Args:\n",
    "            data (dict): Data to be saved\n",
    "            filename (str): Name of the file to save the data\n",
    "        Returns: None\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.looger.info(\"Getting data from API\")\n",
    "            channels = []\n",
    "            for item in data['items']:\n",
    "                id = item['id']['channelId']\n",
    "                created_date = item['snippet']['publishedAt']\n",
    "                title = item['snippet']['title']\n",
    "                description = item['snippet']['description']\n",
    "                publishTime = item['snippet']['publishTime']\n",
    "                channels.append({\n",
    "                    \"id\": id,\n",
    "                    \"created_date\": created_date,\n",
    "                    \"title\": title,\n",
    "                    \"description\": description,\n",
    "                    \"publishTime\": publishTime\n",
    "                })\n",
    "            data = {\"channels\": channels}\n",
    "            today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "            filename = f\"../data/bronze/bronze_data_{today_date}.json\"\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(data, f, indent=4)\n",
    "            self.looger.info(f\"Data saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            self.looger.error(f\"An error occurred while saving data: {e}\")\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    def extract_statictics(self, filename_in=\"../data/bronze/bronze_data_.json\", \n",
    "                           filename_out=\"../data/silver/statistics_data_.json\"):\n",
    "        \"\"\"\n",
    "        Function to extract statistics from YouTube API and save to JSON file\n",
    "        Args:\n",
    "            today (str): Date for which to extract statistics\n",
    "            filename_in (str): Input filename to read data from\n",
    "            filename_out (str): Output filename to save statistics data\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "            self.looger.info(\"Extracting statistics data from API\")\n",
    "            filename_in = f\"../data/bronze/bronze_data_{today_date}.json\"\n",
    "            with open(filename_in, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            statistics_data = []\n",
    "            for channel in data['channels']:\n",
    "                channel_id = channel['id']\n",
    "                stats_url = \"https://youtube.googleapis.com/youtube/v3/channels\"\n",
    "                params = {\n",
    "                    \"part\": \"statistics\",\n",
    "                    \"id\": channel_id,\n",
    "                    \"key\": self.API_KEY\n",
    "                }\n",
    "                response = requests.get(stats_url, params=params)\n",
    "                if response.status_code == 200:\n",
    "                    stats = response.json()\n",
    "                    if 'items' in stats and len(stats['items']) > 0:\n",
    "                        statistics = stats['items'][0]['statistics']\n",
    "                        statistics_data.append({\n",
    "                            \"channel_id\": channel_id,\n",
    "                            \"viewCount\": statistics.get('viewCount', 0),\n",
    "                            \"subscriberCount\": statistics.get('subscriberCount', 0),\n",
    "                            \"videoCount\": statistics.get('videoCount', 0)\n",
    "                        })\n",
    "\n",
    "                time.sleep(0.1)  \n",
    "\n",
    "            filename_out = f\"../data/bronze/statistics_data_{today_date}.json\"\n",
    "            with open(filename_out, 'w') as f:\n",
    "                json.dump({\"statistics\": statistics_data}, f, indent=4)\n",
    "            self.looger.info(f\"Statistics data saved to {filename_out}\")\n",
    "        except Exception as e:\n",
    "            self.looger.error(f\"An error occurred while extracting statistics: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b7516d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class silver:\n",
    "\n",
    "    looger = logging.getLogger(__name__)\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_data_bronze(self, filename=\"../data/bronze/bronze_data_.json\"):\n",
    "        \"\"\"\n",
    "        Function to load bronze data from JSON file into a pandas DataFrame\n",
    "        Args:\n",
    "            filename (str): Name of the file to load the data from\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing the bronze data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "            filename = f\"../data/bronze/bronze_data_{today_date}.json\"\n",
    "            with open(filename, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            df = pd.json_normalize(data['channels'])\n",
    "            self.looger.info(f\"Bronze data loaded from {filename}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.looger.error(f\"An error occurred while loading bronze data: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def load_statistics_data(self, filename=\"../data/bronze/statistics_data_.json\"):\n",
    "        \"\"\"\n",
    "        Function to load statistics data from JSON file into a pandas DataFrame\n",
    "        Args:\n",
    "            filename (str): Name of the file to load the data from\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing the statistics data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "            filename = f\"../data/bronze/statistics_data_{today_date}.json\"\n",
    "            with open(filename, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            df = pd.json_normalize(data['statistics'])\n",
    "            self.looger.info(f\"Statistics data loaded from {filename}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.looger.error(f\"An error occurred while loading statistics data: {e}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "    def merge_data(self, bronze_df: pd.DataFrame, statistics_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Function to merge bronze data with statistics data\n",
    "        Args:\n",
    "            bronze_df (pd.DataFrame): DataFrame containing bronze data\n",
    "            statistics_df (pd.DataFrame): DataFrame containing statistics data\n",
    "        Returns:\n",
    "            pd.DataFrame: Merged DataFrame\n",
    "        \"\"\"\n",
    "        try:\n",
    "            merged_df = pd.merge(bronze_df, statistics_df, left_on='id', right_on='channel_id', how='inner')\n",
    "            self.looger.info(\"Data merged successfully\")\n",
    "            merged_df.to_csv(f\"../data/silver/silver_data_{datetime.today().strftime('%Y-%m-%d')}.csv\", index=False)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            self.looger.error(f\"An error occurred while merging data: {e}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "    def clean_data(self, merged_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Function to clean the merged DataFrame\n",
    "        Args:\n",
    "            merged_df (pd.DataFrame): Merged DataFrame to be cleaned\n",
    "        Returns:\n",
    "            pd.DataFrame: Cleaned DataFrame\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Data cleaning process started\")\n",
    "            df = merged_df.copy()\n",
    "            df = merged_df[['id', 'created_date', 'title', 'description',\n",
    "                           'viewCount', 'subscriberCount', 'videoCount']]\n",
    "            \n",
    "            df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "            df = df[df['videoCount'].apply(lambda x: int(x) > 100)]\n",
    "            df = df[df['subscriberCount'].apply(lambda x: int(x) > 100)]\n",
    "\n",
    "            logging.info(\"Data cleaning process completed\")\n",
    "            df.to_csv(f\"../data/silver/cleaned_silver_data_{datetime.today().strftime('%Y-%m-%d')}.csv\", index=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred during data cleaning: {e}\")\n",
    "            return pd.DataFrame()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8843ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gold:\n",
    "\n",
    "    looger = logging.getLogger(__name__)\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ed0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Crear instancia y solicitar datos\n",
    "bronze_instance = bronze(q=\"Data Engineering\", regionCode=\"US\", type=\"channel\", maxresults=50)\n",
    "data = bronze_instance.request_data(max_channels=100)  # Obtener hasta 1 millón de canales\n",
    "\n",
    "if data:\n",
    "    bronze_instance.save_data(data)\n",
    "    bronze_instance.extract_statictics()\n",
    "else:\n",
    "    print(\"No se pudieron obtener los datos\")\n",
    "\n",
    "logging.info(\"Bronce process completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver process\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "silver_instance = silver()\n",
    "\n",
    "logging.info(\"Extracting the Bronze file\")\n",
    "bronze_df = silver_instance.load_data_bronze()\n",
    "if bronze_df is not None:\n",
    "    logging.info(\"Bronze file extracted successfully\")\n",
    "\n",
    "logging.info(\"Extract the Statistics file\")\n",
    "statistics_df = silver_instance.load_statistics_data()\n",
    "if statistics_df is not None:\n",
    "    logging.info(\"Statistics extracted succesfully\")\n",
    "\n",
    "logging.info(\"Merged process start\")\n",
    "merged_df = silver_instance.merge_data(bronze_df, statistics_df)\n",
    "if merged_df is not None:\n",
    "    logging.info(\"Dataframe merge successfully\")\n",
    "\n",
    "\n",
    "silver_instance.clean_data(merged_df)\n",
    "\n",
    "logging.info(\"Silver process completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
